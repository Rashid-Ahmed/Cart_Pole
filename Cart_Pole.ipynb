{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('CartPole-v1')\n",
    "obs=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discounted_rewards(rewards_total,rewards,discount_rate):\n",
    "    i=len(rewards)-1\n",
    "    discounted_value=0\n",
    "    while i>=0:\n",
    "        rewards[i]+=discounted_value\n",
    "        discounted_value=rewards[i]*discount_rate\n",
    "        i-=1\n",
    "    i=0\n",
    "    while i<len(rewards):\n",
    "        rewards_total.append(rewards[i])\n",
    "        i+=1\n",
    "        \n",
    "def normalized_rewards(rewards_total):\n",
    "    temp=np.array(rewards_total)\n",
    "    Mean=np.mean(temp)\n",
    "    std=np.std(temp)\n",
    "    for i in range(len(rewards_total)):\n",
    "        rewards_total[i]=(rewards_total[i]-Mean)/std\n",
    "    return rewards_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size=4\n",
    "learning_rate=0.01\n",
    "discount_rate=0.95\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "X=tf.placeholder(tf.float32,shape=[1,input_size],name='X')\n",
    "hidden=tf.contrib.layers.fully_connected(X,10,activation_fn=tf.nn.elu,weights_initializer=initializer)\n",
    "final=tf.contrib.layers.fully_connected(hidden,1,activation_fn=None,weights_initializer=initializer)\n",
    "output_probability=tf.nn.sigmoid(final)\n",
    "x=tf.concat([output_probability,1-output_probability],axis=1)\n",
    "action=tf.multinomial(tf.log(x),1)\n",
    "y=1-tf.to_float(action)\n",
    "error=tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=final)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "grads_and_vars=optimizer.compute_gradients(error)\n",
    "gradients = [grad for grad, variable in grads_and_vars]\n",
    "\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "for grad, variable in grads_and_vars:\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs=env.reset()\n",
    "Input=obs.reshape(1,input_size)\n",
    "with tf.Session() as sess:\n",
    "        init.run()\n",
    "        step=0\n",
    "        while(step<1000):\n",
    "            rewards_total=[]\n",
    "            gradients_total=[]\n",
    "            episodes=10\n",
    "            while episodes>0:\n",
    "                obs=env.reset()\n",
    "                done=False\n",
    "                rewards_episode=[]\n",
    "                i=0       \n",
    "                while i<1000 and done==False:\n",
    "                    A,Gradients= sess.run([action,gradients],feed_dict={X:Input})\n",
    "                    gradients_total.append(Gradients)\n",
    "                    obs,reward,done,info=env.step(A[0][0])\n",
    "                    Input=obs.reshape(1,input_size)\n",
    "                    rewards_episode.append(reward)\n",
    "                    i+=1\n",
    "                print (\"Balanced for \",i,\"steps\")\n",
    "                discounted_rewards(rewards_total,rewards_episode,discount_rate)\n",
    "                episodes-=1\n",
    "            rewards_total=normalized_rewards(rewards_total)\n",
    "            Feed_dict=np.mean(np.multiply(np.array(gradients_total).T,np.array(rewards_total)),axis=1)\n",
    "            sess.run(training_op,feed_dict={gradient_placeholders[0]:Feed_dict[0],gradient_placeholders[1]:Feed_dict[1],gradient_placeholders[2]:Feed_dict[2],gradient_placeholders[3]:Feed_dict[3]})\n",
    "            step+=1\n",
    "        sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
